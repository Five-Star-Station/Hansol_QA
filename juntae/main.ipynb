{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "367175f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jtkim1/anaconda3/envs/llmbrender/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from kiwipiepy import Kiwi\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee7db296",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Kiwi for Korean sentence splitting\n",
    "kiwi = Kiwi()\n",
    "\n",
    "# Load the datasets\n",
    "train_df = pd.read_csv(\"data/train.csv\")\n",
    "test_df = pd.read_csv(\"data/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "298c57d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to split queries into sentences using Kiwi\n",
    "def split_into_sentences(query):\n",
    "    return [item[0] for item in kiwi.split_into_sents(query)]\n",
    "\n",
    "# Apply the function to the test dataset\n",
    "test_df[\"split_queries\"] = test_df[\"질문\"].map(split_into_sentences)\n",
    "\n",
    "# Template for generating prompts\n",
    "PROMPT_TEMPLATE = \"\"\"<s>[INST] <<SYS>>\n",
    "You are an helpful assistant for construction and interior. Your task is to generate a valid answer based on the given information:\n",
    "\n",
    "{context}\n",
    "\n",
    "<</SYS>>\n",
    "{query} [/INST]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "425cd72d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XLMRobertaForSequenceClassification(\n",
       "  (roberta): XLMRobertaModel(\n",
       "    (embeddings): XLMRobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): XLMRobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x XLMRobertaLayer(\n",
       "          (attention): XLMRobertaAttention(\n",
       "            (self): XLMRobertaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): XLMRobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): XLMRobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): XLMRobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): XLMRobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_model_name = \"osung-station/deco_embedding\"\n",
    "rerank_model_name = \"Dongjin-kr/ko-reranker\"\n",
    "final_embedding_model_name = (\n",
    "    \"sentence-transformers/distiluse-base-multilingual-cased-v1\"\n",
    ")\n",
    "final_embedding_model = SentenceTransformer(final_embedding_model_name)\n",
    "\n",
    "# Set up the vector store for embeddings\n",
    "chromadb_store = \"vector_store/\"\n",
    "vectordb = Chroma(\n",
    "    persist_directory=chromadb_store,\n",
    "    embedding_function=HuggingFaceEmbeddings(model_name=embedding_model_name),\n",
    ")\n",
    "\n",
    "# # Load reranking model and tokenizer\n",
    "rerank_tokenizer = AutoTokenizer.from_pretrained(rerank_model_name)\n",
    "rerank_model = AutoModelForSequenceClassification.from_pretrained(rerank_model_name)\n",
    "rerank_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d3c630d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to normalize scores\n",
    "def exp_normalize(scores):\n",
    "    b = scores.max()\n",
    "    y = np.exp(scores - b)\n",
    "    return y / y.sum()\n",
    "\n",
    "\n",
    "# Function to get the best matching document\n",
    "def get_first_place_doc(query, matching_docs):\n",
    "    pairs = [[query, doc.page_content] for doc in matching_docs]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        inputs = rerank_tokenizer(\n",
    "            pairs, padding=True, truncation=True, return_tensors=\"pt\", max_length=512\n",
    "        )\n",
    "        logits = rerank_model(**inputs, return_dict=True).logits.view(-1).float()\n",
    "        scores = exp_normalize(logits.numpy())\n",
    "        max_idx = scores.argmax()\n",
    "        #print(scores[max_idx])\n",
    "\n",
    "    return matching_docs[max_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "389a4d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing the queries from the test data\n",
    "embeddings = []\n",
    "generated_answers = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b31913fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  60%|▌| 3/5 [00:21<00:14,  7.18s/it]"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"yanolja/EEVE-Korean-Instruct-10.8B-v1.0\", torch_dtype=torch.bfloat16, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a8cae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prompt(content, query):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"A chat between a curious user and an artificial intelligence assistant. The assistant gives helpful, detailed, and polite answers to the user's questions.\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"\"\"information만 참고해서 question에 답해주세요.\\n\\n\n",
    "            ```information : {first_place_doc.page_content.split(\"answer: \")[1]}\\n\\n question : {query}```\"\"\"\n",
    "        }\n",
    "    ]\n",
    "    return tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49940da",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "for queries in tqdm(test_df[\"split_queries\"][:3]):\n",
    "    generated_answer = []\n",
    "    for query in tqdm(queries):\n",
    "        # Get matching documents for the query\n",
    "        matching_docs = vectordb.similarity_search(query, k=10)\n",
    "        # Find the best document\n",
    "        first_place_doc = get_first_place_doc(query, matching_docs)\n",
    "        \n",
    "        prompt = get_prompt(first_place_doc.page_content.split(\"answer: \")[1], query)\n",
    "        input_text = tokenizer(prompt, return_tensors='pt').to(\"cuda\")\n",
    "            \n",
    "        # Generate Text\n",
    "        outputs = model.generate(\n",
    "            **input_text, \n",
    "            max_new_tokens=256, \n",
    "            pad_token_id=32000, \n",
    "            num_beams=3, \n",
    "            top_p=0.92, \n",
    "            top_k =50, \n",
    "            do_sample=True, \n",
    "            temperature=0.1,\n",
    "            early_stopping=True,\n",
    "            repetition_penalty=1.5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            encoder_repetition_penalty=0.8\n",
    "        )\n",
    "        text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "        \n",
    "        generated_answer.append(text)\n",
    "\n",
    "        #generated_answer.append(first_place_doc.page_content.split(\"answer: \")[1])\n",
    "\n",
    "    generated_answer = \" \".join(generated_answer)\n",
    "    embeddings.append(final_embedding_model.encode(generated_answer))\n",
    "    generated_answers.append(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4764adf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"generated_answer\"] = generated_answers\n",
    "test_df.to_csv(\"generated_test.csv\", index=False)\n",
    "\n",
    "ids = test_df[\"id\"]\n",
    "columns = [\"id\"] + [f\"vec_{i}\" for i in range(512)]\n",
    "data = [[id] + list(embedding) for id, embedding in zip(ids, embeddings)]\n",
    "\n",
    "submission = pd.DataFrame(data, columns=columns)\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392ee72d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llmblender",
   "language": "python",
   "name": "llmbrender"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
