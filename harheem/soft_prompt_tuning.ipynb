{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "from torch.nn.functional import cross_entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = 'bert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForMaskedLM.from_pretrained(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new maximum length for the embeddings from BERT\n",
    "BERT_MAX_LENGTH = 502\n",
    "# Define the new soft prompt length\n",
    "SOFT_PROMPT_LENGTH = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=BERT_MAX_LENGTH):\n",
    "        self.encodings = tokenizer(texts, truncation=True, padding='max_length', max_length=max_length, return_tensors='pt')\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return {key: val[idx] for key, val in self.encodings.items()}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.encodings.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from itertools import product\n",
    "\n",
    "train = pd.read_csv(\"data/train.csv\").sample(100)\n",
    "\n",
    "question_columns = ['질문_1']\n",
    "answer_columns = ['답변_2']\n",
    "\n",
    "queries = []\n",
    "answers = []\n",
    "for question, answer in product(question_columns, answer_columns):\n",
    "    for index, row in train.iterrows():\n",
    "        queries.append(row[question])\n",
    "        answers.append(row[answer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine queries and answers into training data\n",
    "training_data = [f\"Query: {q} Answer: {a}\" for q, a in zip(queries, answers)]\n",
    "\n",
    "# Create a dataset and data loader\n",
    "dataset = SimpleDataset(training_data, tokenizer)\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6525, -0.9803,  1.0921,  ...,  0.3349,  0.5240, -0.3715],\n",
       "        [ 0.1660, -0.2549, -0.2168,  ..., -0.8566,  0.0027, -0.2683],\n",
       "        [-0.0462,  0.0218,  0.4449,  ...,  0.3407,  0.6583, -0.6654],\n",
       "        ...,\n",
       "        [ 0.0522,  0.8773, -0.4474,  ..., -0.1889, -0.0091,  0.2379],\n",
       "        [ 0.9105, -0.1946, -0.9873,  ...,  0.3898,  0.3661, -0.3900],\n",
       "        [ 0.2153,  0.5887,  0.9717,  ..., -0.3128, -0.4339, -0.6346]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's define a soft prompt\n",
    "soft_prompt = torch.nn.Embedding(SOFT_PROMPT_LENGTH, model.config.hidden_size)\n",
    "soft_prompt.weight.data.normal_(mean=0.0, std=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use a simple optimizer just for the soft prompt\n",
    "optimizer = AdamW(soft_prompt.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Loss: 4.761325836181641\n",
      "Epoch 0, Loss: 5.644777297973633\n",
      "Epoch 0, Loss: 4.193082809448242\n",
      "Epoch 0, Loss: 4.40958833694458\n",
      "Epoch 0, Loss: 5.350088596343994\n",
      "Epoch 1, Loss: 4.69485330581665\n",
      "Epoch 1, Loss: 5.628662109375\n",
      "Epoch 1, Loss: 4.254441261291504\n",
      "Epoch 1, Loss: 4.384392738342285\n",
      "Epoch 1, Loss: 5.291342735290527\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "# Training loop for soft prompt tuning\n",
    "for epoch in range(2): \n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        soft_prompt_tokens = torch.arange(SOFT_PROMPT_LENGTH).unsqueeze(0).expand(input_ids.size(0), -1)\n",
    "        soft_prompt_embeddings = soft_prompt(soft_prompt_tokens)\n",
    "        inputs_embeds = model.bert.embeddings.word_embeddings(input_ids)\n",
    "        inputs_embeds = torch.cat((soft_prompt_embeddings, inputs_embeds), dim=1)\n",
    "        attention_mask = torch.cat([torch.ones(input_ids.size(0), SOFT_PROMPT_LENGTH), attention_mask], dim=1)\n",
    "        outputs = model(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "        \n",
    "        # Compute loss\n",
    "        logits = outputs.logits[:, SOFT_PROMPT_LENGTH:, :]  # Exclude the logits for the soft prompt tokens\n",
    "        loss = cross_entropy(logits.reshape(-1, model.config.vocab_size), input_ids.view(-1))\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        print(f\"Epoch {epoch}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the soft prompt embeddings\n",
    "torch.save(soft_prompt.state_dict(), 'path_to_save_soft_prompt/soft_prompt.pt')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harheem-EtTwlMNZ",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
